= Smart Cities Workshop Deployment Guide
:sectnums:
:sectnumlevels: 2
:toc:

== Requirements

* A working OpenShift environment. Last tested version: 4.10
* Proper SSL certificates must be in place for secured Routes
* Cluster-admin access to OpenShift, or ability to deploy operators.
* The image-registry Operator needs to be setup and working
* The `oc` client.
* Optional: the `jq` tool (JSON patcher).

== Automated install

NOTE: This still need to be reviewed

=== Requirements

Follow these steps to ensure you have the right Ansible version and all requirements.

Clone this repository, then cd into it's directory

```bash
git clone https://github.com/red-hat-data-services/jumpstart-library.git
cd jumpstart-library
```

Setup the python venv

```bash
python3 -m venv .venv --system-site-packages
source .venv/bin/activate
```

Then install the dependencies with:

```bash
pip install -r requirements.txt
```

=== Start the install

After you installed all requirements, you can start the install.

Login to your cluster and check with

```bash
oc get nodes
```

Now make Ansible work for you

```bash
ansible-playbook smartcity-playbook.yaml
```

== Manual install

NOTE: All provided commands are to be run from the `deployment` folder after having cloned this repo. Adjust paths if run from another location.

=== Operators

If not already available in your OpenShift deployment, install the following operators from OperatorHub:

* Red Hat OpenShift Data Foundation
* Red Hat AMQ Streams (all namespaces)

=== Deploy OpenShift Data Foundation
- Follow the standard operator deployment
- Create a StorageSystem with a Full Deployment, using available storage class
- You can create a small scale cluster (3 replicas of 0.5TB) as the demo does not require much
- Depending on your OCP environment, you might need to manually deploy ODF RGW. For example, on AWS, the RGW is not deployed by default. https://red-hat-storage.github.io/ocs-training/training/ocs4/ocs4-enable-rgw.html[Read this procedure] to enable the RGW. Only the steps 1 to 4 are needed.

**NOTE**: Please make sure that the `cephobjectstore.yaml` file provided in the referenced documentation uses the right `failureDomain` parameters (there are two instances of them). In the case of an RHPDS instance of OpenShift, this must be set to `host`.

=== OpenShift/ODF Patch

To circumvent some issues in Trino (and other tools) related to bucket access-style (DNS vs Path), we have to make some changes to the ODF deployment.

=== Deploy CoreDNS into the `openshift-storage` namespace:

.Create the odf-coredns ConfigMap
[source,bash]
----
oc apply -f ocp_odf_patch/cm_coredns.yaml
----

.Create the odf-coredns Deployment
[source,bash]
----
oc apply -f ocp_odf_patch/dp_coredns.yaml
----

.Create the odf-coredns Service
[source,bash]
----
oc apply -f ocp_odf_patch/svc_coredns.yaml
----

=== Patch the `dns.operator/default` object from OpenShift

Here we are adding a new zone, `data.local` to our environment.

.Patching command
[source,bash]
----
oc patch dns.operator/default --type=merge --patch '{"spec":{"servers":[{"forwardPlugin":{"upstreams":["'$(oc get -n openshift-storage svc | grep dns | awk '{print $3}')':5353"]},"name":"rook-dns","zones":["data.local"]}]}}'
----

=== Add the new endpoint to the RGW zone configuration

The RGW must know about this new zone it will serve from.

* If not already done, deploy the Ceph toolbox

[source,bash]
----
oc patch OCSInitialization ocsinit -n openshift-storage --type json --patch  '[{ "op": "replace", "path": "/spec/enableCephTools", "value": true }]'
----

* Add the endpoint

IMPORTANT: The next command does everything in one step. If you want to understand what's going on (or just don't trust those long commands...), detailed instructions are available in the next section.

.One line command
[source,bash]
----
oc exec -n openshift-storage deployment/rook-ceph-tools -- bash -c "radosgw-admin zonegroup get --rgw-zonegroup=ocs-storagecluster-cephobjectstore > /tmp/config.json && sed -i 's/\"hostnames\": \[],/\"hostnames\": \[\"s3\.data\.local\",\"*\.s3\.data\.local\",\"rook-ceph-rgw-ocs-storagecluster-cephobjectstore\.openshift-storage\.svc\",\""$(oc get routes -n openshift-storage | grep rgw | awk '{ print $2 }' | sed s/\\./\\\\./g)"\"],/' /tmp/config.json && radosgw-admin zonegroup set --rgw-zonegroup=ocs-storagecluster-cephobjectstore --infile=/tmp/config.json && radosgw-admin period update --commit"
----

=== Manual steps to add the endpoint (Optional)

IMPORTANT: skip this step if you have applied the one-liner configuration at the previous step.

*** Get the current config

[source,bash]
----
echo $(oc exec -n openshift-storage deployment/rook-ceph-tools -- radosgw-admin zonegroup get --rgw-zonegroup=ocs-storagecluster-cephobjectstore) > config.json
----

** Edit the file config.json

In the file `config.json` you obtained, replace the first occurence of `"hostnames": [],` by `"hostnames": ["s3.data.local","*.s3.data.local", "replace here with the Route name for the RGW","rook-ceph-rgw-ocs-storagecluster-cephobjectstore.openshift-storage.svc"],`.

You can also use jq to do that:
`jq '.hostnames = ["s3.data.local","*.s3.data.local", "replace here with the Route name for the RGW","rook-ceph-rgw-ocs-storagecluster-cephobjectstore.openshift-storage.svc"]' config.json > tmp.json && mv tmp.json config.json` (the complicated part with tmp.json is because json cannot edit in place...).

** Upload the modified file to the toolbox

[source,bash]
----
oc rsync . $(oc get pods -n openshift-storage | grep rook-ceph-tools | grep Running | awk '{print $1}'):/tmp --exclude=* --include=config.json --no-perms
----

** Apply the new configuration

[source,bash]
----
oc exec -n openshift-storage deploy/rook-ceph-tools -- radosgw-admin zonegroup set --rgw-zonegroup=ocs-storagecluster-cephobjectstore --infile=/tmp/config.json
oc exec -n openshift-storage deploy/rook-ceph-tools -- radosgw-admin period update --commit
----

=== Namespace

Create an OpenShift project/namespace to deploy the environment. In this documentation we'll use `smartcity`.

[source,bash]
----
oc new-project smartcity
----

TIP: If you did not use `smartcity` as the name of your project, don't forget to change it in the commands or the config files used for the deployment.

=== Deployment

From the `deployment` folder and subfolders, create the OpenShift resources in this order.

.Creating a resource
[source,bash]
----
oc apply -f file.yaml
----

=== Database

We will need a database to store information about the workflow, as well as registration information for the vehicles. You can edit the Secret file if you want change the default values.

Deploying PostgreSQL DB

.Secrets to deploy the PostgreSQL database
[source,bash]
----
oc apply -f database/postgresql/secret_postgresql.yaml
----

.Deployment of the PostgreSQL helper database
[source,bash]
----
oc apply -f database/postgresql/dc_postgresql.yaml
----

.Service for PostgreSQL helper database
[source,bash]
----
oc apply -f database/postgresql/service_postgresql.yaml
----

Deploying Seed Database to initialize the database with the registration information.

.ImageStream for the image that will be used to see the DB
[source,bash]
----
oc apply -f database/seed_database/is_seed_database.yaml
----

.BuildConfiguration for the image
[source,bash]
----
oc apply -f database/seed_database/bc_seed_database.yaml
----

IMPORTANT: Before you apply `job_seed_database.yaml` make sure the build process (from the last step) has been completed, else seed job will complain until the image is not ready.

.Seeding Job to initialize the DB
[source,bash]
----
oc apply -f database/seed_database/job_seed_database.yaml
----

=== Kafka

We will need two different Kafka instances. One will simulate the "Edges", the toll station, the other one the "Core". We will also create the different topics that are needed, as well as the Kafka Mirror Maker to replicate the topics from the Edge to the Core.

.Edge Kafka instance
[source,bash]
----
oc apply -f kafka/edge.yaml
----

.Core Kafka instance
[source,bash]
----
oc apply -f kafka/core.yaml
----

IMPORTANT: Before you create  edge and core kafka topics,  make sure both kafka clusters are up and running.


.Edge topic
[source,bash]
----
oc apply -f kafka/edge-topic.yaml
----

.Core topic
[source,bash]
----
oc apply -f kafka/core-topic.yaml
----

.Mirror maker
[source,bash]
----
oc apply -f kafka/mirror-maker.yaml
----

.Optional! Kafdrop is a UI interface to your Kafka cluster (to inspect messages)
[source,bash]
----
oc apply -f kafka/edge-kafdrop.yaml
----

.Optional! Kafdrop is a UI interface to your Kafka cluster (to inspect messages)
[source,bash]
----
oc apply -f kafka/core-kafdrop.yaml
----

=== LPR Service

This component presents an API that you can query with an image and returns the infered licence plate number.

.ImageStream for the LPR service
[source,bash]
----
oc apply -f lpr_service/is_lpr_service.yaml
----

.BuildConfiguration for the LPR service
[source,bash]
----
oc apply -f lpr_service/bc_lpr_service.yaml
----

.Deployment Configuration for the LPR service
[source,bash]
----
oc apply -f lpr_service/dc_lpr_service.yaml
----

.Service to access the LPR service
[source,bash]
----
oc apply -f lpr_service/svc_lpr_service.yaml
----


=== Events Service

This is the component that runs in the Core and listens to incoming Kafka events to write them into a PostgreSQL database so that they can be queried to create the dashboards.

.ImageStream for the event service
[source,bash]
----
oc apply -f events_service/is_events_service.yaml
----

.BuildConfiguration for the event service
[source,bash]
----
oc apply -f events_service/bc_events_service.yaml
----

.Deployment Configuration for the event service
[source,bash]
----
oc apply -f events_service/dc_events_service.yaml
----

=== Image Server

This component will return the image of the last identified vehicle to be displayed on the dashboard.

.ImageStream for the image-server
[source,bash]
----
oc apply -f image_server/is_image-server.yaml
----

.Build Config for the image-server
[source,bash]
----
oc apply -f image_server/bc_image-server.yaml
----

.Deployment Config/Service/Route for the image-server
[source,bash]
----
sed "s@RGW_SERVICE_ENDPOINT@https://"$(oc get routes -n openshift-storage | grep rgw | awk '{ print $2 }')"@" image_server/dc_image-server.yaml | oc apply -f -
----

=== Load Generator

This is the component that injects car images into the pipeline.

.Bucket to store the images dataset
[source,bash]
----
oc apply -f generator/obc_dataset_generator.yaml
----

.ImageStream for the load generator
[source,bash]
----
oc apply -f generator/is_generator.yaml
----

.BuildConfiguration to create the load generator image
[source,bash]
----
oc apply -f generator/bc_generator.yaml
----

.Deployment Configuration for the load generator
[source,bash]
----
oc apply -f generator/dc_generator.yaml
----

=== Dataset

Retrieve the information for the dataset bucket created previously and upload the images.

[source,bash]
----
export AWS_ACCESS_KEY_ID=$(oc get secret/generator-dataset -o yaml | grep " AWS_ACCESS_KEY_ID" | awk '{ print $2 }' - | base64 -d)
export AWS_SECRET_ACCESS_KEY=$(oc get secret/generator-dataset -o yaml | grep " AWS_SECRET_ACCESS_KEY" | awk '{ print $2 }' - | base64 -d)
export RGW_ROUTE=https://$(oc get routes -n openshift-storage | grep rgw | awk '{ print $2 }')
export BUCKET=$(oc get cm/generator-dataset -o yaml | grep " BUCKET_NAME:" | awk '{ print $2 }' -)
aws --endpoint-url $RGW_ROUTE s3 cp --recursive ../source/dataset/images s3://$BUCKET/images
----

This bucket also has to be made readable to display the images.

.Apply the anonymous readonly policy
[source,bash]
----
sed 's/MY_BUCKET/'$BUCKET'/' image_server/policy.json > /tmp/policy.json && aws --endpoint-url $RGW_ROUTE s3api put-bucket-policy --bucket $BUCKET --policy file:///tmp/policy.json
----

=== Start Traffic

By default `generator` has no pods running, in order to simulate traffic, you will increase the replica count of generator deployment to `1` (not yet, after you have deployed all the components!).

[source,bash]
----
oc scale dc/generator --replicas 1
----

Verify the generated traffic by visiting the following kafdrop URL for edge and core kafka clusters.

[source,bash]
----
echo "http://$(oc get route | grep -i edge-kafdrop | awk '{print $2}')/topic/lpr/messages?partition=0&offset=0&count=100&keyFormat=DEFAULT&format=DEFAULT"
echo "http://$(oc get route | grep -i core-kafdrop | awk '{print $2}')/topic/lpr/messages?partition=0&offset=0&count=100&keyFormat=DEFAULT&format=DEFAULT"
----

=== Fee Calculation

For calculating the toll and pollution fee, there are two cases that we have covered:

* When any vehicle enters the ULEZ, a certain fee (aka toll fee) must be applied to that vehicle
* If the vehicle model is too old (older than 2014), apply addition fee (aka pollution fee) on that vehicle

Deploy the fee calculation component, using the following files:

.ImageStream
[source,bash]
----
oc apply -f fee_calculation/is_fee_calculation.yaml
----

.Build Config
[source,bash]
----
oc apply -f fee_calculation/bc_fee_calculation.yaml
----

.Cron job to launch fee calculations
[source,bash]
----
oc apply -f fee_calculation/cronjob_fee_calculation.yaml
----

=== Secor

Secor is the component that will listen to the Kafka Stream and write the aggregated data to an object Bucket.

.Bucket to store the streamed data
[source,bash]
----
oc apply -f secor/1_obc_secor.yaml
----

.Connection to the Kafka-Core instance
[source,bash]
----
oc apply -f secor/2_zookeeper_entrance.yaml
----

.Deploys the Secor instance
[source,bash]
----
oc apply -f secor/3_secor.yaml
----

=== Trino

Deploy Trino.

[source,bash]
----
oc apply -f trino/trino.yaml
----

Once the components are running (check the pods!) you can connect to the Trino dashboard using its Route. It can be found in the OpenShift UI or like this:

[source,bash]
----
echo "https://$(oc get route | grep -i trino | awk '{print $2}')"
----

=== Superset

Deploy Superset using the following YAML:

.Deploys Superset with all its dependencies
[source,bash]
----
oc apply -f superset/superset.yaml
----

* Transfer the DataSources configuration file into the Superset pod.

[source,bash]
----
oc rsync superset/config $(oc get pod | grep superset- | awk '{print $1}'):/tmp
----

* Import the datasources into Superset (PostgreSQL and Hive from Trino):

[source,bash]
----
oc exec $(oc get pod | grep superset- | awk '{print $1}') -- superset import_datasources -p /tmp/config/superset-datasources.yaml
oc exec $(oc get pod | grep superset- | awk '{print $1}') -- superset set_database_uri -d "PostgreSQL" -u postgresql://dbadmin:dbpassword@smartcity-db-service/pgdb
----

* Log into Superset with OpenShift authentication.

* From the Settings menu (top right), import the example dashboard from the file `dashboard/dashboard.json` into the PostgreSQL database.

==== Datasets creation with Trino

Once the trino-coordinator pod is running, connect to it:

[source,bash]
----
oc rsh $(oc get pod | grep trino-coordinator | awk '{print $1}')
----

From the pod prompt, connect to trino server:

[source,bash]
----
trino --server localhost:8080 --catalog hive --schema default
----

From the Trino prompt, create schema and table.

IMPORTANT: Before you execute the command to create schema and table , make sure to replace the bucket name with your bucket. To grab bucket name execute `oc get obc secor-obc -o json | jq -r .spec.bucketName`

[source,sql]
----
CREATE SCHEMA hive.odf WITH (location = 's3a://replace_with_secor_bucket_name/');

CREATE TABLE IF NOT EXISTS hive.odf.event(event_timestamp timestamp, event_id varchar, event_vehicle_detected_plate_number varchar, event_vehicle_detected_lat varchar, event_vehicle_detected_long varchar, event_vehicle_lpn_detection_status varchar, stationa1 boolean, stationa5201 boolean, stationa13 boolean, stationa2 boolean, stationa23 boolean, stationb313 boolean, stationa4202 boolean, stationa41 boolean, stationb504 boolean, dt varchar) with ( external_location = 's3a://replace_with_secor_bucket_name/raw_logs/lpr/', format = 'ORC', partitioned_by=ARRAY['dt']);

CALL system.sync_partition_metadata(schema_name=>'odf', table_name=>'event', mode=>'FULL');

SELECT event_timestamp,event_vehicle_detected_plate_number,event_vehicle_lpn_detection_status FROM hive.odf.event LIMIT 10;
----

You should see a table with the result of the query.
You can then exit trino with `exit`, then the Pod itself with `exit` again.

=== Grafana

First, deploy the **Grafana Operator** in the `smartcity` namespace using the OperatorHub.

Once the operator is deployed in the namespace, create a grafana instance.

.Deploys the Grafana instance
[source,bash]
----
oc apply -f grafana/grafana.yaml
----

Grafana will allow us to create dashboards to visualize the data workflow (Ops dashboard) and the Business Application itself (Main dashboard). All the deployments are taken care of by the Grafana operator.

* PGSQL Source to retrieve the events and vehicle data:

.Retrieve the secrets, process the template, and apply the configuration
[source,bash]
----
oc process -f grafana/grafana-pgsql-datasource.yaml -p db_database=$(oc get secret/postgresql -o yaml | grep " database-name:" | awk '{ print $2 }' - | base64 -d) -p db_user=$(oc get secret/postgresql -o yaml | grep " database-user:" | awk '{ print $2 }' - | base64 -d) -p db_password=$(oc get secret/postgresql -o yaml | grep " database-password:" | awk '{ print $2 }' - | base64 -d) | oc apply -f -
----

* Prometheus Data Source to retrieve the CPU and RAM metrics

Our Grafana dashboard will connect to the main OpenShift Prometheus instance to retrieve CPU and RAM information. To enable this, follow those steps:

.Grant the Grafana Service Account the cluster-monitoring-view cluster role:
[source,bash]
----
oc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana-serviceaccount
----

.Retrieve the bearer token used to authenticate to Prometheus:
[source,bash]
----
export bearer_token=$(oc get secret $(oc get secret | grep grafana-serviceaccount-token | awk '{print $1}') -o go-template --template="{{.data.token|base64decode}}")
----

.Deploy the Prometheus data source by using the template and substituting the bearer token:
[source,bash]
----
sed 's/BEARER_TOKEN/'$bearer_token'/' grafana/grafana-prometheus-datasource.yaml | oc apply -f -
----

You can now apply the dashboard files:

* Main application dashboard

.Retrieve the image server url, process the template, and apply the configuration
[source,bash]
----
oc process -f grafana/grafana-main-dashboard.yaml -p image_server_host=$(oc get route | grep -i image-server | awk '{print $2}') | oc apply -f -
----

.CPU Ops dashboard
[source,bash]
----
oc apply -f grafana/grafana-pipeline-cpu-dashboard.yaml
----

.RAM Ops dashboard
[source,bash]
----
oc apply -f grafana/grafana-pipeline-ram-dashboard.yaml
----

You can now connect to Grafana to see the dashboard. The Route can be retrieved with:

[source,bash]
----
echo "https://$(oc get routes -n smartcity | grep grafana | awk '{ print $2 }')"
----

The default login is admin / secret.
